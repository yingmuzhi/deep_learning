{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3. Attention Scoring Functions\n",
    "\n",
    "As it turns out, distance functions are slightly more expensive to compute than inner products.\n",
    "\n",
    "我们简化Q和K的二范数得到了点积注意力机制。用点积来表示注意力权重大大减小了乘方带来的计算开销。\n",
    "\n",
    "#### 11.3.2.1. Masked Softmax Operation\n",
    "\n",
    "我们引入了Mask操作，为了保证NLP中的Sequence保持相同的长度，这是因为你一个序列有时候词数量不同，但需要并行化处理。我们通过将注意力权重值在softmax前（即QK内积）后置为一个无穷小来实现，例如10的负六次方。这样是为了方便GPU并行计算快（if-else开销更大）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5468, 0.4532, 0.0000, 0.0000],\n",
      "         [0.6060, 0.3940, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.4260, 0.1756, 0.3984, 0.0000],\n",
      "         [0.4944, 0.1954, 0.3102, 0.0000]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3492, 0.2424, 0.4084, 0.0000]],\n",
       "\n",
       "        [[0.4861, 0.5139, 0.0000, 0.0000],\n",
       "         [0.3200, 0.2312, 0.2007, 0.2480]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "def masked_softmax(X, valid_lens):  #@save\n",
    "    \"\"\"Perform softmax operation by masking elements on the last axis.\"\"\"\n",
    "    # X: 3D tensor, valid_lens: 1D or 2D tensor\n",
    "    def _sequence_mask(X, valid_len, value=0):\n",
    "        maxlen = X.size(1)\n",
    "        mask = torch.arange((maxlen), dtype=torch.float32,\n",
    "                            device=X.device)[None, :] < valid_len[:, None]\n",
    "        X[~mask] = value\n",
    "        return X\n",
    "\n",
    "    if valid_lens is None:\n",
    "        return nn.functional.softmax(X, dim=-1)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if valid_lens.dim() == 1:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
    "        else:\n",
    "            valid_lens = valid_lens.reshape(-1)\n",
    "        # On the last axis, replace masked elements with a very large negative\n",
    "        # value, whose exponentiation outputs 0\n",
    "        X = _sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)\n",
    "        return nn.functional.softmax(X.reshape(shape), dim=-1)\n",
    "\n",
    "print(masked_softmax(torch.rand(2, 2, 4), torch.tensor([2, 3])))\n",
    "masked_softmax(torch.rand(2, 2, 4), torch.tensor([[1, 3], [2, 4]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3.2.2. Batch Matrix Multiplication\n",
    "\n",
    "when we have minibatches of queries, keys, and values.  # be careful that Q's dimensions == d 即Q的dimension才是人们常说的d  \n",
    "\n",
    "    - Shape of queries: (batch_size, no. of queries, d)\n",
    "    - Shape of keys: (batch_size, no. of key-value pairs, d)\n",
    "    - Shape of values: (batch_size, no. of key-value pairs, value dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "Q = torch.ones((2, 3, 4))\n",
    "K = torch.ones((2, 4, 6))\n",
    "print(torch.bmm(Q, K).shape) # (2, 3, 6))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.3. Scaled Dot-Product Attention\n",
    "\n",
    "除以了根号d， 这个d就是上面说的Q的d，不用记，不求甚解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "class DotProductAttention(nn.Module):  #@save\n",
    "    \"\"\"Scaled dot product attention.\"\"\"\n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # Shape of queries: (batch_size, no. of queries, d)\n",
    "    # Shape of keys: (batch_size, no. of key-value pairs, d)\n",
    "    # Shape of values: (batch_size, no. of key-value pairs, value dimension)\n",
    "    # Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\n",
    "    def forward(self, queries, keys, values, valid_lens=None):\n",
    "        d = queries.shape[-1]\n",
    "        # Swap the last two dimensions of keys with keys.transpose(1, 2)\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)\n",
    "    \n",
    "\n",
    "queries = torch.normal(0, 1, (2, 1, 2))\n",
    "keys = torch.normal(0, 1, (2, 10, 2))\n",
    "values = torch.normal(0, 1, (2, 10, 4))\n",
    "valid_lens = torch.tensor([2, 6])\n",
    "\n",
    "attention = DotProductAttention(dropout=0.5)\n",
    "attention.eval()\n",
    "print(attention(queries, keys, values, valid_lens).shape)   #, (2, 1, 4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.4. Additive Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 11.3.4. Additive Attention"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_cp311_ymz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00e71849280e25957c1d4fbefef47541ff0643b96feeb23b86f5c961ecde847d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
