{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.7. The Transformer Architecture\n",
    "\n",
    "Notably, self-attention enjoys both parallel computation and the shortest maximum path length. 自注意力机制拥有并行计算能力和最短路径长度的优势。\n",
    "\n",
    "the Transformer model is solely based on attention mechanisms without any convolutional or recurrent layer Transformer 模型没有使用任何卷积层和循环层。\n",
    "\n",
    " in a wide range of modern deep learning applications, such as in areas of language, vision, speech, and reinforcement learning.虽然刚开始Transformer主要是用于序列2序列的，但是现在广泛用于语言，视觉，演讲和强化学习。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.7.1. Model\n",
    "\n",
    "与引入Bahdanau attention的Seq2Seq不同，虽然他俩都是encoder-decorder架构。但是Transformer是在encoder前就引入了position encoding位置信息。\n",
    "\n",
    "The first is a multi-head self-attention pooling and the second is a positionwise feed-forward network. 在encoder架构中每一个block由两个sublayers子层组成，第一个是多头自注意力聚集，第二个是位置前馈神经网络。\n",
    "\n",
    "Specifically, in the encoder self-attention, queries, keys, and values are all from the outputs of the previous encoder layer.每一个encoder模块都是顺序执行的，`input -> position encoding -> multi-head attention -> residual add -> layer norm -> FFN -> residual add -> layer norm`. 整个过程的dimension都为d.\n",
    "\n",
    "decoder部分由3个sublayers组成，增加了一层mask。In the encoder-decoder attention, queries are from the outputs of the previous decoder layer, and the keys and values are from the Transformer encoder outputs. In the decoder self-attention, queries, keys, and values are all from the outputs of the previous decoder layer. 其中第一层attention的qkv来自上一层的decoder输出，第二层attention的q来自第一层attention输出，kv来自endoer的输出。 However, each position in the decoder is allowed to only attend to all positions in the decoder up to that position. This masked attention preserves the auto-regressive property, ensuring that the prediction only depends on those output tokens that have been generated. 但是在decoder时，我们限制了 position encoding 并引入了 mask ，这样是为了保证 auto-regressive 自回归，即此处推理用不到后面的数据。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.7.2. Positionwise Feed-Forward Networks\n",
    "\n",
    "The positionwise feed-forward network transforms the representation at all the sequence positions using the same MLP. This is why we call it positionwise 位置逐点前馈网络使用相同的多层感知机（MLP）来转换序列中所有位置的表示。这就是为什么我们称之为位置逐点。MLP由两层线性层组成。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFFN(nn.Module):  #@save\n",
    "    \"\"\"The positionwise feed-forward network.\"\"\"\n",
    "    def __init__(self, ffn_num_hiddens, ffn_num_outputs):\n",
    "        super().__init__()\n",
    "        self.dense1 = nn.LazyLinear(ffn_num_hiddens)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.LazyLinear(ffn_num_outputs)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.dense2(self.relu(self.dense1(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/env_cp311_ymz/lib/python3.11/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5735, -0.0443,  0.7113, -1.1059,  0.2718,  0.4457,  0.1448, -0.5916],\n",
       "        [ 1.5735, -0.0443,  0.7113, -1.1059,  0.2718,  0.4457,  0.1448, -0.5916],\n",
       "        [ 1.5735, -0.0443,  0.7113, -1.1059,  0.2718,  0.4457,  0.1448, -0.5916]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn = PositionWiseFFN(4, 8)\n",
    "ffn.eval()\n",
    "ffn(torch.ones((2, 3, 4)))[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.7.3. Residual Connection and Layer Normalization\n",
    "\n",
    "BatchNorm是针对通道来计算均值方差；但是Sequence的长度往往不确定(可变长度序列，如`\"hello world <blank> <blank>\"`和`\"my name is yingmuzhi\"`他俩的长度就不同，所以LN是针对一个sample实例进行归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer norm: tensor([[-1.0000,  1.0000],\n",
      "        [-1.0000,  1.0000]], grad_fn=<NativeLayerNormBackward0>) \n",
      "batch norm: tensor([[-1.0000, -1.0000],\n",
      "        [ 1.0000,  1.0000]], grad_fn=<NativeBatchNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# BN和LN举例\n",
    "ln = nn.LayerNorm(2)\n",
    "bn = nn.LazyBatchNorm1d()\n",
    "X = torch.tensor([[1, 2], [2, 3]], dtype=torch.float32)\n",
    "# Compute mean and variance from X in the training mode\n",
    "print('layer norm:', ln(X), '\\nbatch norm:', bn(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):  #@save\n",
    "    \"\"\"The residual connection followed by layer normalization.\"\"\"\n",
    "    def __init__(self, norm_shape, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(norm_shape)\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        return self.ln(self.dropout(Y) + X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4]) (2, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "add_norm = AddNorm(4, 0.5)\n",
    "shape = (2, 3, 4)\n",
    "print(add_norm(torch.ones(shape), torch.ones(shape)).shape, shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.7.4. Encoder\n",
    "\n",
    "使用一层encoder\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):  #@save\n",
    "    \"\"\"The Transformer encoder block.\"\"\"\n",
    "    def __init__(self, num_hiddens, ffn_num_hiddens, num_heads, dropout,\n",
    "                 use_bias=False):\n",
    "        super().__init__()\n",
    "        # self.attention = d2l.MultiHeadAttention(num_hiddens, num_heads,\n",
    "        #                                         dropout, use_bias)\n",
    "        self.attention = d2l.MultiHeadAttention(num_hiddens,num_hiddens,num_hiddens, num_hiddens, num_heads,\n",
    "                                                dropout, use_bias)\n",
    "        self.addnorm1 = AddNorm(num_hiddens, dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_hiddens, num_hiddens)\n",
    "        self.addnorm2 = AddNorm(num_hiddens, dropout)\n",
    "\n",
    "    def forward(self, X, valid_lens):\n",
    "        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))\n",
    "        return self.addnorm2(Y, self.ffn(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 100, 24]) torch.Size([2, 100, 24])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/env_cp311_ymz/lib/python3.11/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "X = torch.ones((2, 100, 24))\n",
    "valid_lens = torch.tensor([3, 2])\n",
    "encoder_blk = TransformerEncoderBlock(24, 48, num_heads=8, dropout=0.5)\n",
    "encoder_blk.eval()\n",
    "print(encoder_blk(X, valid_lens).shape, X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(d2l.Encoder):  #@save\n",
    "    \"\"\"The Transformer encoder.\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens,\n",
    "                 num_heads, num_blks, dropout, use_bias=False):\n",
    "        super().__init__()\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_blks):\n",
    "            self.blks.add_module(\"block\"+str(i), TransformerEncoderBlock(\n",
    "                num_hiddens, ffn_num_hiddens, num_heads, dropout, use_bias))\n",
    "\n",
    "    def forward(self, X, valid_lens):\n",
    "        # Since positional encoding values are between -1 and 1, the embedding\n",
    "        # values are multiplied by the square root of the embedding dimension\n",
    "        # to rescale before they are summed up\n",
    "        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))\n",
    "        self.attention_weights = [None] * len(self.blks)\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X = blk(X, valid_lens)\n",
    "            self.attention_weights[\n",
    "                i] = blk.attention.attention.attention_weights\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 100, 24]) (2, 100, 24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/env_cp311_ymz/lib/python3.11/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "encoder = TransformerEncoder(200, 24, 48, 8, 2, 0.5)\n",
    "print(encoder(torch.ones((2, 100), dtype=torch.long), valid_lens).shape,\n",
    "                (2, 100, 24))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.7.5. Decoder\n",
    "\n",
    "decoder由三个sublayers组成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderBlock(nn.Module):\n",
    "    # The i-th block in the Transformer decoder\n",
    "    def __init__(self, num_hiddens, ffn_num_hiddens, num_heads, dropout, i):\n",
    "        super().__init__()\n",
    "        self.i = i\n",
    "        self.attention1 = d2l.MultiHeadAttention(num_hiddens,num_hiddens,num_hiddens,\n",
    "            num_hiddens, num_heads,\n",
    "                                                 dropout)\n",
    "        self.addnorm1 = AddNorm(num_hiddens, dropout)\n",
    "        self.attention2 = d2l.MultiHeadAttention(num_hiddens,num_hiddens,num_hiddens,\n",
    "            num_hiddens, num_heads,\n",
    "                                                 dropout)\n",
    "        self.addnorm2 = AddNorm(num_hiddens, dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_hiddens, num_hiddens)\n",
    "        self.addnorm3 = AddNorm(num_hiddens, dropout)\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        enc_outputs, enc_valid_lens = state[0], state[1]\n",
    "        # During training, all the tokens of any output sequence are processed\n",
    "        # at the same time, so state[2][self.i] is None as initialized. When\n",
    "        # decoding any output sequence token by token during prediction,\n",
    "        # state[2][self.i] contains representations of the decoded output at\n",
    "        # the i-th block up to the current time step\n",
    "        if state[2][self.i] is None:\n",
    "            key_values = X\n",
    "        else:\n",
    "            key_values = torch.cat((state[2][self.i], X), dim=1)\n",
    "        state[2][self.i] = key_values\n",
    "        if self.training:\n",
    "            batch_size, num_steps, _ = X.shape\n",
    "            # Shape of dec_valid_lens: (batch_size, num_steps), where every\n",
    "            # row is [1, 2, ..., num_steps]\n",
    "            dec_valid_lens = torch.arange(\n",
    "                1, num_steps + 1, device=X.device).repeat(batch_size, 1)\n",
    "        else:\n",
    "            dec_valid_lens = None\n",
    "        # Self-attention\n",
    "        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)\n",
    "        Y = self.addnorm1(X, X2)\n",
    "        # Encoder-decoder attention. Shape of enc_outputs:\n",
    "        # (batch_size, num_steps, num_hiddens)\n",
    "        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)\n",
    "        Z = self.addnorm2(Y, Y2)\n",
    "        return self.addnorm3(Z, self.ffn(Z)), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(torch.Size([2, 100, 24]), torch.Size([2, 100, 24]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/env_cp311_ymz/lib/python3.11/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "decoder_blk = TransformerDecoderBlock(24, 48, 8, 0.5, 0)\n",
    "X = torch.ones((2, 100, 24))\n",
    "state = [encoder_blk(X, valid_lens), valid_lens, [None]]\n",
    "print((decoder_blk(X, state)[0].shape, X.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(d2l.AttentionDecoder):\n",
    "    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads,\n",
    "                 num_blks, dropout):\n",
    "        super().__init__()\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_blks = num_blks\n",
    "        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_blks):\n",
    "            self.blks.add_module(\"block\"+str(i), TransformerDecoderBlock(\n",
    "                num_hiddens, ffn_num_hiddens, num_heads, dropout, i))\n",
    "        self.dense = nn.LazyLinear(vocab_size)\n",
    "\n",
    "    def init_state(self, enc_outputs, enc_valid_lens):\n",
    "        return [enc_outputs, enc_valid_lens, [None] * self.num_blks]\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))\n",
    "        self._attention_weights = [[None] * len(self.blks) for _ in range (2)]\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X, state = blk(X, state)\n",
    "            # Decoder self-attention weights\n",
    "            self._attention_weights[0][\n",
    "                i] = blk.attention1.attention.attention_weights\n",
    "            # Encoder-decoder attention weights\n",
    "            self._attention_weights[1][\n",
    "                i] = blk.attention2.attention.attention_weights\n",
    "        return self.dense(X), state\n",
    "\n",
    "    @property\n",
    "    def attention_weights(self):\n",
    "        return self._attention_weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.7.6. Training\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.5.1. Downloading and Preprocessing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'd2l.torch' has no attribute 'HyperParameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mDataModule\u001b[39;00m(d2l\u001b[39m.\u001b[39;49mHyperParameters):\n\u001b[1;32m      2\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"The base class of data.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[39m    Defined in :numref:`subsec_oo-design-models`\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, root\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m../data\u001b[39m\u001b[39m'\u001b[39m, num_workers\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'd2l.torch' has no attribute 'HyperParameters'"
     ]
    }
   ],
   "source": [
    "class HyperParameters:\n",
    "    \"\"\"The base class of hyperparameters.\"\"\"\n",
    "    def save_hyperparameters(self, ignore=[]):\n",
    "        \"\"\"Defined in :numref:`sec_oo-design`\"\"\"\n",
    "        raise NotImplemented\n",
    "\n",
    "    def save_hyperparameters(self, ignore=[]):\n",
    "        \"\"\"Save function arguments into class attributes.\n",
    "    \n",
    "        Defined in :numref:`sec_utils`\"\"\"\n",
    "        frame = inspect.currentframe().f_back\n",
    "        _, _, _, local_vars = inspect.getargvalues(frame)\n",
    "        self.hparams = {k:v for k, v in local_vars.items()\n",
    "                        if k not in set(ignore+['self']) and not k.startswith('_')}\n",
    "        for k, v in self.hparams.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "class DataModule(d2l.HyperParameters):\n",
    "    \"\"\"The base class of data.\n",
    "\n",
    "    Defined in :numref:`subsec_oo-design-models`\"\"\"\n",
    "    def __init__(self, root='../data', num_workers=4):\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def get_dataloader(self, train):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.get_dataloader(train=True)\n",
    "\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.get_dataloader(train=False)\n",
    "\n",
    "\n",
    "    def get_tensorloader(self, tensors, train, indices=slice(0, None)):\n",
    "        \"\"\"Defined in :numref:`sec_synthetic-regression-data`\"\"\"\n",
    "        tensors = tuple(a[indices] for a in tensors)\n",
    "        dataset = torch.utils.data.TensorDataset(*tensors)\n",
    "        return torch.utils.data.DataLoader(dataset, self.batch_size,\n",
    "                                           shuffle=train)\n",
    "\n",
    "class MTFraEng(DataModule):  #@save\n",
    "    \"\"\"The English-French dataset.\"\"\"\n",
    "    def _download(self):\n",
    "        d2l.extract(d2l.download(\n",
    "            d2l.DATA_URL+'fra-eng.zip', self.root,\n",
    "            '94646ad1522d915e7b0f9296181140edcf86a4f5'))\n",
    "        with open(self.root + '/fra-eng/fra.txt', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "\n",
    "data = MTFraEng()\n",
    "raw_text = data._download()\n",
    "print(raw_text[:75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'd2l.torch' has no attribute 'MTFraEng'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[39m=\u001b[39m d2l\u001b[39m.\u001b[39;49mMTFraEng(batch_size\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m)\n\u001b[1;32m      2\u001b[0m num_hiddens, num_blks, dropout \u001b[39m=\u001b[39m \u001b[39m256\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0.2\u001b[39m\n\u001b[1;32m      3\u001b[0m ffn_num_hiddens, num_heads \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m, \u001b[39m4\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'd2l.torch' has no attribute 'MTFraEng'"
     ]
    }
   ],
   "source": [
    "data = d2l.MTFraEng(batch_size=128)\n",
    "num_hiddens, num_blks, dropout = 256, 2, 0.2\n",
    "ffn_num_hiddens, num_heads = 64, 4\n",
    "encoder = TransformerEncoder(\n",
    "    len(data.src_vocab), num_hiddens, ffn_num_hiddens, num_heads,\n",
    "    num_blks, dropout)\n",
    "decoder = TransformerDecoder(\n",
    "    len(data.tgt_vocab), num_hiddens, ffn_num_hiddens, num_heads,\n",
    "    num_blks, dropout)\n",
    "model = d2l.Seq2Seq(encoder, decoder, tgt_pad=data.tgt_vocab['<pad>'],\n",
    "                    lr=0.0015)\n",
    "trainer = d2l.Trainer(max_epochs=30, gradient_clip_val=1, num_gpus=1)\n",
    "trainer.fit(model, data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_cp311_ymz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00e71849280e25957c1d4fbefef47541ff0643b96feeb23b86f5c961ecde847d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
