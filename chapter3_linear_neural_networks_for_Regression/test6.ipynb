{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataModule类有一个父类(HyperParameters)\n",
    "\n",
    "在DataModule类定义中，主要有以下方法\n",
    "\n",
    "- `__init__` \n",
    "\n",
    "传入数据的url或者数据的根目录文件。downloading the data and preprocessing the data.\n",
    "\n",
    "亦可以自己定义数据集(但仍然建议这一步在DIYDataset类中完成)\n",
    "```python\n",
    "self.X = torch.randn(n, len(w))\n",
    "noise = torch.randn(n, 1) * noise\n",
    "self.y = torch.matmul(self.X, w.reshape((-1, 1))) + b + noise \n",
    "```\n",
    "- `def train_dataloader` 调用`def get_dataloader`并传入train==True，返回train的DataLoader。returns data loader for the training set.\n",
    "- `def val_dataloader(option)` 调用`def get_dataloader`并传入train==False，返回validation的DataLoader。returns data loader for the validaton set.\n",
    "- `def get_dataloader` 调用`def get_tensorloader`并传入是否是训练集，返回DataLoader\n",
    "- `def get_tensorloader` 根据train标记生成对应dataset=Dataset的子类DIYDataset，并根据Dataset生成DataLoader，返回DataLoader\n",
    "\n",
    "数据究竟如何`__getitem__`和`__len__`取决于DIYDataset类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Intro:\n",
    "    The DataModule class is the base class for data.\n",
    "\n",
    "    __init__ method is used to prepare the data. This includes downloading and preprocessing if needed.\n",
    "\n",
    "    train_dataloader returns the data loader for the training dataset. A data loader is a (Python) generator that yields a data batch each time it is used. This batch is then fed into the training_step method of Module to compute the loss.\n",
    "\n",
    "    There is an optional val_dataloader to return the validation dataset loader.\n",
    "\"\"\"\n",
    "class DataModule(HyperParameters):  #@save\n",
    "    \"\"\"The base class of data.\"\"\"\n",
    "    def __init__(self, root='../data', num_workers=4):\n",
    "        \"\"\"\n",
    "        intro:\n",
    "            read the data path.\n",
    "        \"\"\"\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \"\"\"\n",
    "        intro:\n",
    "            return training dataloader\n",
    "        \"\"\"\n",
    "        return self.get_dataloader(train=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        \"\"\"\n",
    "        intro:\n",
    "            return validation dataloader\n",
    "        \"\"\"\n",
    "        return self.get_dataloader(train=False)\n",
    "    \n",
    "    def get_dataloader(self, train):\n",
    "        \"\"\"\n",
    "        intro:\n",
    "            return train / validation dataloader depend on train == True / False\n",
    "        \"\"\"\n",
    "        # raise NotImplementedError\n",
    "        i = slice(0, self.num_train) if train else slice(self.num_train, None)\n",
    "        return self.get_tensorloader((self.X, self.y), train, i)\n",
    "\n",
    "    # add\n",
    "    def get_tensorloader(self, tensors, train, indices=slice(0, None)):\n",
    "        \"\"\"\n",
    "        intro:\n",
    "            get dataset through `class DIYDataset` inherit `Dataset` using `torch.utils.data.Dataset`. then return dataloader\n",
    "        \"\"\"\n",
    "        tensors = tuple(a[indices] for a in tensors)\n",
    "        dataset = torch.utils.data.TensorDataset(*tensors)\n",
    "        return torch.utils.data.DataLoader(dataset, self.batch_size,\n",
    "                                           shuffle=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module类有两个父类(torch.nn.Module, HyperParameters)\n",
    "\n",
    "在Module类模型定义中，主要有以下几个方法\n",
    "\n",
    "- `def __init__`: \n",
    "    \n",
    "    定义模型参数，self.w, self.b或者self.net; \n",
    "    \n",
    "    初始化模型参数; \n",
    "    \n",
    "    生成绘制loss的图片，实例化ProgressBoard()类==self.board\n",
    "- `def forward`:  模型参数和数据的计算方式，如何对参数进行训练\n",
    "- `def loss`:     损失函数。传入signal_hat和target计算并返回loss。\n",
    "- `def configure_optimizers`: 书写优化函数，作用在于如何使用算法使得loss最小，且更新参数。\n",
    "- `def training_step` 在训练阶段接收一个batch的数据，这里的batch是已经训练好的batch，并将batch数据分为signal_hat和target传入`def loss`，计算两者的loss并返回，同时使用plot传入这个batch中的loss值。accepts a data batch to return the loss value.\n",
    "- `def validation_step(option)` 测试阶段使用。evaluation measures.\n",
    "- `def plot` 绘制一个epoch-loss曲线，但是会有一个参数叫做`self.plot_train_per_epoch`决定了一个epoch中有几个点会在图上绘制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Name: \n",
    "    Model module\n",
    "\n",
    "Intro: \n",
    "    The Module class is the base class of all models we will implement. It inherits from `nn.Module`\n",
    "\n",
    "    __init__, stores the learnable parameters\n",
    "\n",
    "    training_step method accepts a data batch to return the loss value\n",
    "\n",
    "    configure_optimizers returns the optimization method, or a list of them, that is used to update the learnable parameters\n",
    "\n",
    "    validation_step to report the evaluation measures.\n",
    "\"\"\"\n",
    "class Module(nn.Module, HyperParameters):  #@save\n",
    "    \"\"\"The base class of models.\"\"\"\n",
    "    def __init__(self, plot_train_per_epoch=2, plot_valid_per_epoch=1):\n",
    "        \"\"\"\n",
    "        intro:\n",
    "            init self.net to get model parameters. \n",
    "            init self.board to get ProgressBar, that is, TensorBoard.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = None\n",
    "        self.board = ProgressBoard()\n",
    "\n",
    "    def loss(self, y_hat, y):\n",
    "        \"\"\"\n",
    "        intro:\n",
    "            init loss(fn), then calculate the loss between (y_hat, y).\n",
    "        \"\"\"\n",
    "        # raise NotImplementedError\n",
    "        fn = nn.MSELoss()\n",
    "        return fn(y_hat, y)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        intro:\n",
    "            how to calculate the model learnable parameters using forward.\n",
    "        \"\"\"\n",
    "        assert hasattr(self, 'net'), 'Neural network is defined'\n",
    "        return self.net(X)\n",
    "\n",
    "    def plot(self, key, value, train):\n",
    "        \"\"\"\n",
    "        intro:\n",
    "            plot loss. Plot a point in animation.\n",
    "        \"\"\"\n",
    "        assert hasattr(self, 'trainer'), 'Trainer is not inited'\n",
    "        self.board.xlabel = 'epoch'\n",
    "        if train:\n",
    "            x = self.trainer.train_batch_idx / \\\n",
    "                self.trainer.num_train_batches\n",
    "            n = self.trainer.num_train_batches / \\\n",
    "                self.plot_train_per_epoch\n",
    "        else:\n",
    "            x = self.trainer.epoch + 1\n",
    "            n = self.trainer.num_val_batches / \\\n",
    "                self.plot_valid_per_epoch\n",
    "        self.board.draw(x, value.to(cpu()).detach().numpy(),\n",
    "                        ('train_' if train else 'val_') + key,\n",
    "                        every_n=int(n))\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        \"\"\"\n",
    "        intro:\n",
    "            how to calculate loss in one batch.\n",
    "        \"\"\"\n",
    "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
    "        self.plot('loss', l, train=True)\n",
    "        return l\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        \"\"\"\n",
    "        intro:\n",
    "            how to calculate loss in one batch.\n",
    "        \"\"\"\n",
    "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
    "        self.plot('loss', l, train=False)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        intro:\n",
    "            return optimizer\n",
    "        \"\"\"\n",
    "        # raise NotImplementedError\n",
    "        return torch.optim.SGD(self.parameters(), self.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer类有一个父类(HyperParameters)\n",
    "\n",
    "在Trainer类定义中，主要有以下几个方法\n",
    "\n",
    "- `def __init__` 初始化一些训练参数。\n",
    "\n",
    "指定训练过程中的max_epochs, \n",
    "\n",
    "使用哪个GPU训练num_gpus,\n",
    "\n",
    " 梯度是否裁剪以及对应裁剪值gradient_clip_val\n",
    "- `def fit` 传入Module的实例model，DataModule实例data，进行训练。\n",
    "\n",
    "调用`def prepare_data`\n",
    "\n",
    "调用`def prepare_model`\n",
    "\n",
    "调用`model.configure_optimizers`指定self.optim。使用module中的类方法返回值为trainer中的优化算法\n",
    "\n",
    "初始化self.epoch, self.train_batch_idx, self.val_batch_idx\n",
    "\n",
    "每一个epoch调用`def fit_epoch()`\n",
    "\n",
    "- `def prepare_data` 传入data，即DataModule实例类。预处理在DIYDataset中或者更早已经完成。使用self.train_dataloader = data.train_dataloader()来给trainer的train_dataloader赋值。\n",
    "- `def prepare_model` 传入Module实例类。指定trainer.model=model, model.trainer=trainer, model.board.xlim为trainer.max_epochs\n",
    "- `def fit_epoch` 一个epoch所要做的事情"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Name:\n",
    "    Training Module\n",
    "\n",
    "Intro:\n",
    "    The Trainer class trains the learnable parameters in the Module class with data specified in DataModule. \n",
    "\n",
    "    The key method is fit, which accepts two arguments: model, an instance of Module, and data, an instance of DataModule. It then iterates over the entire dataset max_epochs times to train the model. \n",
    "\"\"\"\n",
    "class Trainer(HyperParameters):  \n",
    "    \"\"\"The base class for training models with data.\"\"\"\n",
    "    def __init__(self, max_epochs, num_gpus=0, gradient_clip_val=0):\n",
    "        \"\"\"\n",
    "        intro:\n",
    "            init trainer\n",
    "        \"\"\"\n",
    "        self.save_hyperparameters()\n",
    "        assert num_gpus == 0, 'No GPU support yet'\n",
    "\n",
    "    def prepare_data(self, data):\n",
    "        \"\"\"\n",
    "        intro:\n",
    "            get self.train_dataloader, self.val_dataloader\n",
    "        \"\"\"\n",
    "        self.train_dataloader = data.train_dataloader()\n",
    "        self.val_dataloader = data.val_dataloader()\n",
    "        try:\n",
    "            self.num_train_batches = len(self.train_dataloader) if len(self.train_dataloader) else 0\n",
    "            self.num_val_batches = (len(self.val_dataloader)\n",
    "                                if self.val_dataloader is not None else 0)\n",
    "        except:\n",
    "            print(\"not using torch dataset\")\n",
    "    \n",
    "    # add\n",
    "    def prepare_batch(self, batch):\n",
    "        return batch\n",
    "    \n",
    "    def prepare_model(self, model):\n",
    "        \"\"\"\n",
    "        intro:\n",
    "            get self.model\n",
    "        \"\"\"\n",
    "        model.trainer = self\n",
    "        model.board.xlim = [0, self.max_epochs]\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, model, data):\n",
    "        \"\"\"\n",
    "        intro:\n",
    "            fit.\n",
    "        \"\"\"\n",
    "        self.prepare_data(data)\n",
    "        self.prepare_model(model)\n",
    "        self.optim = model.configure_optimizers()\n",
    "        self.epoch = 0\n",
    "        self.train_batch_idx = 0\n",
    "        self.val_batch_idx = 0\n",
    "        for self.epoch in range(self.max_epochs):\n",
    "            self.fit_epoch()\n",
    "\n",
    "    def fit_epoch(self):\n",
    "        \"\"\"\n",
    "        intro:\n",
    "            fit per epoch\n",
    "        \"\"\"\n",
    "        # 1. training time\n",
    "        self.model.train()\n",
    "        for batch in self.train_dataloader:\n",
    "            loss = self.model.training_step(self.prepare_batch(batch))\n",
    "            self.optim.zero_grad()\n",
    "            # 2. grad clip\n",
    "            with torch.no_grad():\n",
    "                loss.backward()\n",
    "                if self.gradient_clip_val > 0:\n",
    "                    self.clip_gradients(self.gradient_clip_val, self.model)\n",
    "                self.optim.step()\n",
    "            self.train_batch_idx += 1\n",
    "        if self.val_dataloader is None:\n",
    "            return \n",
    "        # 3. (optional) validation time\n",
    "        self.model.eval()\n",
    "        for batch in self.val_dataloader:\n",
    "            with torch.no_grad():\n",
    "                self.model.validation_step(self.prepare_batch(batch))\n",
    "            self.val_batch_idx += 1"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
