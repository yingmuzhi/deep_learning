{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## x.1 使用torch对张量进行数据处理 \n",
    "\n",
    "在第一章中我们主要讲解了tensor的创建，在这第三章节中我们主要讲解如何对这些tensor进行数据处理，主要是在Calculus基础上在DL的主要运用，即Automatic Differentiation自动微分的处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## x.2 矩阵各种乘法\n",
    "\n",
    "Pytorch中转置矩阵使用transpose(...)或者permute(...).contiguous()方法，参考`https://blog.csdn.net/qq_43369406/article/details/130004509`\n",
    "\n",
    "在Pytorch中各种乘法：参考`https://zhuanlan.zhihu.com/p/514053520`\n",
    "\n",
    "1. 矩阵乘法\n",
    "\n",
    "矩阵乘法包括向量点积，矩阵矩阵乘法，矩阵向量乘法，矩阵A矩阵B乘法要求B的列数等于A的行数。\n",
    "\n",
    "其中一维向量和一维向量的**点积**用`torch.dot()`;\n",
    "\n",
    "**矩阵-向量乘法**用`torch.mv()`;\n",
    "\n",
    "**矩阵-矩阵乘法**用`torch.mm()`或者`@`;\n",
    "\n",
    "2. 哈达玛积\n",
    "\n",
    "哈达玛积要求两个矩阵的行和列相等。哈达玛积是卷积操作中最常见的操作，也即矩阵对应位置的元素相乘。\n",
    "\n",
    "**哈达玛积**用`*`;\n",
    "\n",
    "`*`是向量元素中逐个元素相乘;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(4, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "tensor([[ 0,  4,  8],\n",
      "        [ 1,  5,  9],\n",
      "        [ 2,  6, 10],\n",
      "        [ 3,  7, 11]])\n",
      "torch.mm() result is \n",
      "tensor([[ 14,  38,  62],\n",
      "        [ 38, 126, 214],\n",
      "        [ 62, 214, 366]])\n",
      "tensor(28.) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(12).reshape(3, 4)\n",
    "b = torch.transpose(a, 0, 1)\n",
    "print(a, b, sep='\\n')\n",
    "# result = torch.dot(a, b) wrong\n",
    "# print(result)\n",
    "print(\"torch.mm() result is \\n{}\".format(torch.mm(a, b)))\n",
    "\n",
    "y = 2 * torch.dot(x, x)\n",
    "print(y, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## x.7 automatic differentiation\n",
    "\n",
    "x.requires_grad_(True)增加追踪梯度信息\n",
    "\n",
    "y.backward()反向传播，注意只可以对标量进行反向传播。该步骤会更新需要追踪的参数的梯度矩阵\n",
    "\n",
    "x.grad查看梯度矩阵\n",
    "\n",
    "x.grad.zero_()  将梯度信息设置为0，不然梯度信息会累积。常见如在下一epoch中累积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4, dtype=torch.float32)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当将tensor的requires_grad指定为True后，将会增加required_grad关键字进行梯度自动追踪；\n",
    "\n",
    "在python中的True和False是大写，可以试做常量写法。True == 1; 但是在C语言中就是小写的，如bool flag = false;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can also create x = torch.arange(4.0, requires_grad=True)\n",
    "x.requires_grad_(True)\n",
    "print(x.grad)  # The gradient is None by default\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们虽然使用requires_grad追踪了梯度，使用了dot进行计算，但是x.grad并没有值，我们首先需要使用y.backward()进行反向传播才能有值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28., grad_fn=<MulBackward0>) \n",
      " None\n"
     ]
    }
   ],
   "source": [
    "y = 2 * torch.dot(x, x)\n",
    "print(y, '\\n', x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()    # 相当于对于每个位置都是对应位置的元素的平方，just like y = 2x^2, y' = 4x, 带入x得值\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在实际计算中，我们每次都要将模型的参数的梯度重新清零在进行计算，否则会引起梯度爆炸，梯度清零使用x.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(6., grad_fn=<SumBackward0>), tensor([1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset the gradient\n",
    "x.grad.zero_()  \n",
    "y = x.sum()\n",
    "y.backward()\n",
    "y, x.grad   # 相当于y = x1 + x2 + ... 对每个变量求偏导数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个完整的反向传播计算梯度的过程如下，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3.], requires_grad=True)::\n",
      "tensor([4., 5., 6., 7.], requires_grad=True)::\n",
      "tensor([0., 0., 0., 0.])::\n",
      "None\n",
      "tensor([4., 5., 6., 7.]) tensor([0., 1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "z = torch.tensor([4, 5, 6, 7], requires_grad=True, dtype=torch.float32) # the same as # z = torch.nn.Parameter(torch.tensor([4, 5, 6, 7], dtype=torch.float32))\n",
    "print(x, z, x.grad, z.grad, sep='::\\n')\n",
    "a = torch.dot(x, z)\n",
    "a.backward()\n",
    "print(x.grad, z.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### x.7.1 非标量变量的反向传播\n",
    "\n",
    "当你对矩阵进行运算时，你最终得到的值必须得是一个标量y，再对这个标量y进行反向传播\n",
    "\n",
    "在使用反向传播度的时候需要注意一点，你进行梯度反向传播的量如a.backward()中的a必须得是一个标量，标量即torch.Tensor的维度为0的量，不带有中括号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.], requires_grad=True),\n",
       " tensor([0., 1., 4., 9.], grad_fn=<MulBackward0>),\n",
       " tensor([0., 2., 4., 6.]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x   # *是向量元素中逐个元素相乘，并不是矩阵乘法，矩阵乘法用torch.mm或者@\n",
    "y.sum().backward()  # 因为最终得是一个标量才能进行反向传播，所以要用sum()对y进行降维，即y=x1*x1 + x2*x2, 否则就是y = [x1*x1, x2*x2]\n",
    "# y.backward(gradient=torch.ones(len(y)))  # Faster: y.sum().backward()\n",
    "x, y, x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### x.7.2 使用detach()剥离计算图\n",
    "\n",
    "注意u = y.detach()是指返回的u是一个从计算图中的剥离的常数，而不是y从计算图中剥离出来了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.], requires_grad=True),\n",
       " tensor([ 0.,  1.,  8., 27.], grad_fn=<MulBackward0>),\n",
       " tensor([0., 1., 4., 9.]),\n",
       " tensor([0., 1., 4., 9.]),\n",
       " tensor([True, True, True, True]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "u = y.detach()\n",
    "z = u * x\n",
    "\n",
    "z.sum().backward()\n",
    "x, z, x.grad, u, x.grad == u"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
